{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iglAqaybDbZe"
      },
      "source": [
        "https://arxiv.org/pdf/2007.08032.pdf - main study behind this paper\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baQwQZV2KUpp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-TmjPh8ZEQG"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Cloning code base to colab....')\n",
        "    !git clone https://github.com/ryannguyen10/fairface.git\n",
        "    !cd fairface/utils && bash train.sh\n",
        "    CODE_ROOT = \"fairface/\"\n",
        "else:\n",
        "    CODE_ROOT = '..'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlQ2wrg5uw_K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import tqdm.notebook as tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix as cm, classification_report as cr\n",
        "from __future__ import print_function, division\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import copy\n",
        "from PIL import ImageFile\n",
        "import random\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import argparse\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('%s/res/'%CODE_ROOT)\n",
        "from models.models import get_model\n",
        "from loader.loader import get_loader\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "sns.set_context(\"poster\")\n",
        "sns.set_palette(\"Set1\", 8, .75)\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "eJpubTOmarbl"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'fairface'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 200\n",
        "ARCH = 'LATE_BRANCHING_COMBINED'\n",
        "\n",
        "image_transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4825, 0.3579, 0.3045], std=[0.2077, 0.1805, 0.1696])\n",
        "    ])\n",
        "\n",
        "GPU = 1\n",
        "NUM_CLASSES = (7,7,7,7)\n",
        "loader_new = get_loader('multi_attribute_loader_file_list_mnist_rotation')\n",
        "\n",
        "file_list_root = '%sdataset_lists'%CODE_ROOT\n",
        "att_path = '%sdataset_lists/combined_attributes.p'%CODE_ROOT\n",
        "shuffles = {'train':True,'val':True,'test':True}\n",
        "data_dir = '%sdata/facial_image/fairface-img-margin025-trainval'%CODE_ROOT\n",
        "file_lists = {}\n",
        "dsets = {}\n",
        "dset_loaders = {}\n",
        "dset_sizes = {}\n",
        "for phase in ['train','val','test']:\n",
        "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,DATASET_NAME)\n",
        "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform, data_dir)\n",
        "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=BATCH_SIZE, shuffle = shuffles[phase], num_workers=2,drop_last=True)\n",
        "    dset_sizes[phase] = len(dsets[phase])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform that normalizes the data\n",
        "#transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load the dataset\n",
        "#train_data = datasets.ImageFolder('/content/fairface/data/facial_image/fairface-img-margin025-trainval', transform=transform)\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "#mean = 0.\n",
        "#std = 0.\n",
        "#for images, _ in train_loader:\n",
        "#    batch_samples = images.size(0)\n",
        "#    images = images.view(batch_samples, images.size(1), -1)\n",
        "#    mean += images.mean(2).sum(0)\n",
        "#    std += images.std(2).sum(0)\n",
        "\n",
        "#mean /= len(train_loader.dataset)\n",
        "#std /= len(train_loader.dataset)\n",
        "\n",
        "#print(\"Mean: \", mean)\n",
        "#print(\"Std: \", std)"
      ],
      "metadata": {
        "id": "Sq3bHZjcNmcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "NOFWuGTUaxBo"
      },
      "outputs": [],
      "source": [
        "multi_losses = [nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss()]\n",
        "def weight_scheduler(epoch_num, task):\n",
        "    if task == 'shared':\n",
        "        return [0.0,1.0,0.0,1.0]\n",
        "    elif task == 'gender':\n",
        "        return [0.0,1.0,0.0,0.0]\n",
        "    elif task == 'race':\n",
        "        return [0.0,0.0,0.0,1.0]\n",
        "def train_epoch(model, task, optimizer):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    phase = 'train'\n",
        "    \n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in tqdm(dset_loaders[phase]):\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "            \n",
        "            phase_epoch_loss += calculated_loss\n",
        "        calculated_loss.backward()\n",
        "        optimizer.step()\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    # print('Train loss:%s'%epoch_loss)\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "\n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'gender':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'race':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    return model, epoch_loss, epoch_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rVWfFq9ta2hr"
      },
      "outputs": [],
      "source": [
        "def test_epoch(model, best_model, best_test_loss, best_test_gm, task):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    phase = 'val'\n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in tqdm(dset_loaders[phase]):\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "        phase_epoch_loss += calculated_loss\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    # print('Test loss:%s'%epoch_loss)\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "    \n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'gender':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'race':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    if epoch_loss < best_test_loss:\n",
        "        best_model = model\n",
        "        best_test_loss = epoch_loss\n",
        "        best_test_gm = epoch_gm\n",
        "\n",
        "    return best_model, epoch_loss, epoch_gm, best_test_loss, best_test_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kd6oivEaa8pJ"
      },
      "outputs": [],
      "source": [
        "def unseen_test_epoch(model, task):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    phase = 'test'\n",
        "\n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in tqdm(dset_loaders[phase]):\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "\n",
        "        phase_epoch_loss += calculated_loss\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "    \n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'gender':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'race':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    return epoch_loss, epoch_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "YW0PWONV1pFa"
      },
      "outputs": [],
      "source": [
        "models = {} \n",
        "models['shared']= get_model(ARCH,NUM_CLASSES)\n",
        "models['gender']= get_model(ARCH,NUM_CLASSES)\n",
        "models['race']= get_model(ARCH,NUM_CLASSES)\n",
        "\n",
        "models['shared'].cuda();\n",
        "models['gender'].cuda();\n",
        "models['race'].cuda();\n",
        "\n",
        "best_models = {}\n",
        "best_models['shared'] = models['shared']\n",
        "best_models['gender'] = models['gender']\n",
        "best_models['race'] = models['race']\n",
        "\n",
        "best_test_loss = 100\n",
        "best_test_gm = 0\n",
        "\n",
        "all_train_gms = {}\n",
        "all_train_gms['shared'] = [0]\n",
        "all_train_gms['separate'] = [0]\n",
        "\n",
        "all_test_gms = {}\n",
        "all_test_gms['shared'] = [0]\n",
        "all_test_gms['separate'] = [0]\n",
        "\n",
        "all_unseen_test_gms = {}\n",
        "all_unseen_test_gms['shared'] = [0]\n",
        "all_unseen_test_gms['separate'] = [0]\n",
        "\n",
        "optimizers = {}\n",
        "optimizers['shared'] = optim.Adam(models['shared'].parameters(), lr=0.001)\n",
        "optimizers['gender'] = optim.Adam(models['gender'].parameters(), lr=0.001)\n",
        "optimizers['race'] = optim.Adam(models['race'].parameters(), lr=0.001)\n",
        "\n",
        "plt.rc('xtick', labelsize=14) \n",
        "plt.rc('ytick', labelsize=14) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zmomXTbsPx4"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1):\n",
        "    train_gm_separate = 1\n",
        "    test_gm_separate = 1\n",
        "    unseen_test_gm_separate = 1\n",
        "\n",
        "    for TASK in ['gender','race','shared']:\n",
        "        print('Epoch: %s, Task: %s'%(epoch,TASK))\n",
        "        print('---------')\n",
        "        models[TASK], train_loss, train_gm = train_epoch(models[TASK], TASK, optimizers[TASK])\n",
        "        best_models[TASK], test_loss, test_gm, best_test_loss, best_test_gm = test_epoch(models[TASK], best_models[TASK], best_test_loss, best_test_gm, TASK)\n",
        "        unseen_test_loss, unseen_test_gm = unseen_test_epoch(models[TASK], TASK)\n",
        "\n",
        "        if TASK != 'shared':\n",
        "            train_gm_separate = train_gm_separate * train_gm\n",
        "            test_gm_separate = test_gm_separate * test_gm\n",
        "            unseen_test_gm_separate = unseen_test_gm_separate * test_gm\n",
        "\n",
        "    all_train_gms['separate'].append(np.sqrt(train_gm_separate))\n",
        "    all_test_gms['separate'].append(np.sqrt(test_gm_separate))\n",
        "    all_unseen_test_gms['separate'].append(np.sqrt(unseen_test_gm_separate))\n",
        "    all_train_gms['shared'].append(train_gm)\n",
        "    all_test_gms['shared'].append(test_gm)\n",
        "    all_unseen_test_gms['shared'].append(np.sqrt(unseen_test_gm))\n",
        "    \n",
        "    clear_output()\n",
        "    print('Epoch: %s'%epoch)\n",
        "    print('---------')\n",
        "    \n",
        "    line_labels = [\"Separate\", \"Shared\"]\n",
        "\n",
        "    fig,ax = plt.subplots(1, 3, figsize=(20,6))\n",
        "    l1 = ax[0].plot(all_train_gms['separate'], color = 'blue', marker = 'o', markersize=5)[0]\n",
        "    l2 = ax[0].plot(all_train_gms['shared'], color = 'red', marker = 'o', markersize=5)[0]\n",
        "    ax[0].set_title('Train Accuracy', fontsize=12)\n",
        "    ax[0].set_ylim(0,1.05)\n",
        "\n",
        "    ax[1].plot(all_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
        "    ax[1].plot(all_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
        "    ax[1].set_title('Test Accuracy on Seen \\n Race-Gender Combinations', fontsize=12)\n",
        "    ax[1].set_ylim(0,1.05)\n",
        "\n",
        "    ax[2].plot(all_unseen_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
        "    ax[2].plot(all_unseen_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
        "    ax[2].set_ylim(0,1.05)\n",
        "    ax[2].set_title('Test Accuracy on Unseen \\n Race-Gender Combinations', fontsize=12)\n",
        "    fig.legend([l1, l2],     # The line objects\n",
        "            labels=line_labels,   # The labels for each line\n",
        "            loc=\"center right\",   # Position of legend\n",
        "            borderaxespad=0.2,    # Small spacing around legend box\n",
        "            prop={\"size\":20})\n",
        "    plt.subplots_adjust(right=0.85)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(all_train_gms['shared'][8])\n",
        "#print(all_train_gms['separate'][8])\n",
        "#print(all_test_gms['shared'][8])\n",
        "#print(all_test_gms['separate'][8])\n",
        "#print(all_unseen_test_gms['shared'][8])\n",
        "#print(all_unseen_test_gms['separate'][8])"
      ],
      "metadata": {
        "id": "DsjkjSAWmk9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(models['gender'].state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyOgYpjRKU2l",
        "outputId": "2d6b6841-de05-4964-81f6-b52374bbec8c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'fc_1.weight', 'fc_1.bias', 'fc_2.weight', 'fc_2.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shared_weights = models['shared'].state_dict()\n",
        "#for name, weight in shared_weights.items():\n",
        "#    print(name, weight.shape)\n",
        "\n",
        "for i in range(64):\n",
        "    img = dsets[i]\n",
        "    plt.imshow(img)\n",
        "\n",
        "shared_weights = models['shared'].state_dict()\n",
        "gender_weights = models['gender'].state_dict()\n",
        "race_weights = models['race'].state_dict()\n",
        "\n",
        "shared_weights = shared_weights['conv1.weight'].permute(0, 2, 3, 1).cpu().numpy()\n",
        "gender_weights = gender_weights['conv1.weight'].permute(0, 2, 3, 1).cpu().numpy()\n",
        "race_weights = race_weights['conv1.weight'].permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "axs[0].imshow(shared_weights, cmap='viridis')\n",
        "axs[0].set_title('Shared Weights')\n",
        "axs[0].set_ylabel('Convolutional Layer 1 Weights')\n",
        "\n",
        "axs[1].imshow(gender_weights, cmap='viridis')\n",
        "axs[1].set_title('Gender Weights')\n",
        "\n",
        "axs[2].imshow(race_weights, cmap='viridis')\n",
        "axs[2].set_title('Race Weights')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jcYuju5habzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}